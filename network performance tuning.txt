
Network performance tuning(on youtube)

How Receive works:
  NIC ->Kernel->Protocols->to App

  Packtet arrives in RingBuffer
  NIC Hard Interrupts, stops a running process, context switches
  Kernel runs an an Interrupt Handler to turn the interrupt off
  Kernel schedules a SoftIRQ to do the actual receive work(visible as ksoftirq/X or %sys CPU usage)
  Data payload is placed in Socket Buffers for apps to recv()
  SoftIRQ continues to do NAPI Polling until quiet

IRQ
  set IRQ banned CPU:
     /etc/default/irqbalance or/etc/sysconfig/irqbalance
  stats: /proc/interrupts
  
Numa locality

#numactl -H

Devices can have NUMA locality too
  cat /sys/class/net/$DEV/device/numa_node
   -1 means no locality, depends on hardware platform

BottleNeck: Network Interface Receive
  Packet loss before the OS
  ethtool -statistics ethX each Vendor's name for "packet loss" varies, look for stats like drop, discard, buffer, fifo
  don't trust pkt drop counter from ifconfig/net-tools

Reasons receive loss can occur:
  Poor Interrupt Handling
  Ring Buffer Overflows
  Lack of Offloading
  Kernel not picking traffic fast enough
  Ethernet Flow Control(aka Pause Frames)

Poor Interrupt Handling
  cat /proc/interrupts
  An array of CPU core and interrupt channels
  Look for spread loads, or CPU0 will starve and miss interrupts
  
Interrputs: Queues
  Mordern network interfaces have more than one receive queue, the number of queues is normally configurable
  ideally have one queue per CPU(within the NUMA Node)
    ethtool --show-ring -eth0
    ethtool --show-time-stamping 
    ethtool --show-features eth0  


Application:
  netstat -s for collapse, pruned
  ss for socke twith data in Recv-Q for a long time
    ss -npt

RedHat Enterprise linux for real time 7 tuning guide:
Hardware clock   
  cat /sys/devices/system/clocksource/clocksource0/available_clocksource 
      tsc hpet acpi_pm 
  idle=poll : Forces the clock to avoid entering the idle state
  processor.max_cstate=1: prevent the clock from entering deeper C-state(energy saving mode)


TCP:
  disable nagle buffering algorithm by using TCP_NODELAY
  disable tcp timestamps
  /proc/sys/net/ipv4/tcp_timestamps

System partitioning
  nohz=on
  nohz_full
  intel_pstate=disable : prevents the intel idle driver from managing power state and CPU frequency
  nosoftlookup: prevents the kernel from detecting soft lookups in user threads
sample config: isolcpus=1-3,5,9-14 nohz=on nohz_full=1-3,5,9-14 intel_pstate=disable nosoftlookup


check thread priority
  ps -T -l pid

Realtime scheduler throttling
  /proc/sys/kernel/sched_rt_runtime_us
  default value is 950,000(0.95s) or in other words, 95% of CPU bandwidth. if a single realtime task occupies that 95% CPU time slot, 
  the remaining realtime tasks on the CPU will not run. the remaining 5%of the is used only by non-realtime tasks. Realtime tasks will
  have at most 95% of CPU time available from them, probalby affecting their performance.



Loading Dynamic Libraries

LD_BIND_NOW=1
export LD_BIND_NOW

chrt --fifo 1 app

